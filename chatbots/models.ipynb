{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArNZkMSSMV79",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import re\n",
        "from google.colab import drive\n",
        "import logging\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft\n",
        "!pip install -U transformers accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "IwQTDHzaP_el",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llzgOtika6HI",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Projects/backend/models\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.read_csv('sufinama_ghazals.csv')\n",
        "df1 = pd.read_csv('rekhta.csv')"
      ],
      "metadata": {
        "id": "qN3TdXiihVLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([df1, df], ignore_index=True, sort=False)"
      ],
      "metadata": {
        "id": "yEOXHUrpRlEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean = df.dropna(subset=[\"ghazal_en\", \"ghazal_ur\"])"
      ],
      "metadata": {
        "id": "Ks9s7bwSRprE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean"
      ],
      "metadata": {
        "id": "WLsm_hf_WmKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Detect Inconsistencies ---\n",
        "def has_hindi(text):\n",
        "    \"\"\"Check for Devanagari (Hindi) characters in Urdu column <button class=\"citation-flag\" data-index=\"1\">\"\"\"\n",
        "    return any('\\u0900' <= char <= '\\u097F' for char in str(text))\n",
        "\n",
        "def invalid_roman(text):\n",
        "    \"\"\"Check if Roman Urdu contains Urdu script <button class=\"citation-flag\" data-index=\"5\">\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return True\n",
        "    return any('\\u0600' <= char <= '\\u06FF' for char in str(text))\n",
        "\n",
        "# Flag script inconsistencies and missing values <button class=\"citation-flag\" data-index=\"5\"><button class=\"citation-flag\" data-index=\"6\">\n",
        "df[\"has_hindi\"] = df[\"ghazal_ur\"].apply(has_hindi)\n",
        "df[\"invalid_roman\"] = df[\"ghazal_en\"].apply(invalid_roman)\n",
        "df[\"missing_ghazal_ur\"] = df[\"ghazal_ur\"].isna()  # Flag original NaNs <button class=\"citation-flag\" data-index=\"5\">\n",
        "df[\"missing_ghazal_en\"] = df[\"ghazal_en\"].isna()  # Flag original NaNs <button class=\"citation-flag\" data-index=\"6\">\n",
        "\n",
        "# Combine all flags to define inconsistent data\n",
        "inconsistent_data = df[\n",
        "    df[\"has_hindi\"] |\n",
        "    df[\"invalid_roman\"] |\n",
        "    df[\"missing_ghazal_ur\"] |\n",
        "    df[\"missing_ghazal_en\"]\n",
        "]\n",
        "\n",
        "# Clean data (rows without inconsistencies or missing values)\n",
        "clean_data = df[\n",
        "    ~(\n",
        "        df[\"has_hindi\"] |\n",
        "        df[\"invalid_roman\"] |\n",
        "        df[\"missing_ghazal_ur\"] |\n",
        "        df[\"missing_ghazal_en\"]\n",
        "    )\n",
        "]\n"
      ],
      "metadata": {
        "id": "Rk9eYBQmR5EA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_data"
      ],
      "metadata": {
        "id": "ESVUNRTMIL7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Save results <button class=\"citation-flag\" data-index=\"6\">\n",
        "inconsistent_data.to_csv(\"inconsistent_data.csv\", index=False)\n",
        "clean_data.to_csv(\"clean_ghazals.csv\", index=False)"
      ],
      "metadata": {
        "id": "H3jZhcPEZEzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df.loc[2294,\"ghazal_url\"]"
      ],
      "metadata": {
        "id": "BW4tzXTXM3H-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[df[\"name\"] == 'baba-shah-hussaini']"
      ],
      "metadata": {
        "id": "njebDdlQfsjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "df = pd.read_csv(\"clean_ghazals.csv\")\n",
        "# Create training examples\n",
        "training_data = []"
      ],
      "metadata": {
        "id": "lkI2EEfJ-VNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"ghazal_ur\"] = df[\"ghazal_ur\"].fillna(\"[MISSING]\")\n",
        "df[\"ghazal_en\"] = df[\"ghazal_en\"].fillna(\"[MISSING]\")"
      ],
      "metadata": {
        "id": "TYlr8JJRAY9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[df['ghazal_ur'] == \"[MISSING]\"]"
      ],
      "metadata": {
        "id": "s7bVXK5VBnqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _, row in df.iterrows():\n",
        "    poet = row[\"name\"].replace(\"-\", \" \").title()\n",
        "\n",
        "    # Task 1: Urdu → Roman Conversion\n",
        "    training_data.append({\n",
        "        \"instruction\": f\"Convert this Urdu ghazal by {poet} to Roman Urdu\",\n",
        "        \"input\": row[\"ghazal_ur\"],\n",
        "        \"response\": row[\"ghazal_en\"]\n",
        "    })\n",
        "\n",
        "    # Task 2: Roman → Urdu Conversion\n",
        "    training_data.append({\n",
        "        \"instruction\": f\"Convert this Roman Urdu ghazal by {poet} to traditional Urdu script\",\n",
        "        \"input\": row[\"ghazal_en\"],\n",
        "        \"response\": row[\"ghazal_ur\"]\n",
        "    })\n",
        "\n",
        "    # Task 3: Poet Attribution\n",
        "    training_data.append({\n",
        "        \"instruction\": \"Who wrote this ghazal?\",\n",
        "        \"input\": row[\"ghazal_ur\"],\n",
        "        \"response\": poet\n",
        "    })"
      ],
      "metadata": {
        "id": "3RPleTvWSPHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_ids = pad_sequence([item[\"input_ids\"] for item in batch], batch_first=True)\n",
        "    attention_mask = pad_sequence([item[\"attention_mask\"] for item in batch], batch_first=True)\n",
        "    labels = pad_sequence([item[\"labels\"] for item in batch], batch_first=True)\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels,\n",
        "    }"
      ],
      "metadata": {
        "id": "vWqfbyTBuNwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip uninstall -y tensorflow && pip install tensorflow-cpu"
      ],
      "metadata": {
        "id": "BDw2ntFJdwyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from accelerate import Accelerator\n",
        "import torch\n",
        "from torch.utils.data import Dataset"
      ],
      "metadata": {
        "id": "R4wnZmiX6Zhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Quantization config\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    llm_int8_threshold=6.0,\n",
        ")\n",
        "\n",
        "# Load model\n",
        "model_name = \"qwen/Qwen2-0.5B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "# model.gradient_checkpointing_enable()\n",
        "\n",
        "class LazyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        example = self.data[idx]\n",
        "        inputs = example[\"instruction\"] + \" \" + example[\"input\"]\n",
        "        tokenized = tokenizer(\n",
        "            text=inputs,\n",
        "            text_pair=example[\"response\"],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=256,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        labels = tokenized[\"input_ids\"].clone()\n",
        "        labels[labels == tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": tokenized[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": tokenized[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": labels.squeeze(0)  # Include labels\n",
        "        }\n",
        "\n",
        "\n",
        "# Prepare data\n",
        "dataset = LazyDataset(training_data, tokenizer)\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=2,\n",
        "    collate_fn=collate_fn  # Add this\n",
        ")\n",
        "# Accelerate setup\n",
        "accelerator = Accelerator()\n",
        "optimizer = torch.optim.AdamW(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=5e-5\n",
        ")\n",
        "model, optimizer, train_dataloader = accelerator.prepare(\n",
        "    model, optimizer, train_dataloader\n",
        ")"
      ],
      "metadata": {
        "id": "Bsep0PEVdE4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_dataloader:\n",
        "    print(\"Labels shape:\", batch[\"labels\"].shape)  # Should match input_ids shape\n",
        "    break"
      ],
      "metadata": {
        "id": "HQ8niIqgvTO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./qwen-urdu-test\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    fp16=True,\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=10,\n",
        "    save_steps=100,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "# Resume training from the last checkpoint\n",
        "trainer.train(resume_from_checkpoint=True)"
      ],
      "metadata": {
        "id": "xVuF-YcYi0gI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "# Load with proper tokenizer settings\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"qwen/Qwen2-0.5B\",\n",
        "    pad_token=\"<|endoftext|>\",  # Qwen's EOS token\n",
        "    padding_side=\"left\"  # Crucial for generation\n",
        ")\n",
        "\n",
        "# Load base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"qwen/Qwen2-0.5B\",\n",
        "    device_map=\"auto\",\n",
        "    pad_token_id=tokenizer.pad_token_id  # Match tokenizer\n",
        ")\n",
        "\n",
        "# Load LoRA adapter\n",
        "model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    \"./qwen-urdu-test/checkpoint-76533\"  # Your specific checkpoint\n",
        ")\n",
        "\n",
        "# Critical configuration for generation\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "def generate_safe(prompt, max_length=200):\n",
        "    # Tokenize with attention mask\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    ).to(model.device)\n",
        "\n",
        "    # Generate with proper config\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs.input_ids,\n",
        "        attention_mask=inputs.attention_mask,  # Pass mask\n",
        "        max_new_tokens=max_length,\n",
        "        temperature=0.7,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id\n",
        "    )\n",
        "\n",
        "    # Decode only new tokens\n",
        "    input_length = inputs.input_ids.shape[1]\n",
        "    return tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "2w6awAmkK0zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_urdu_features():\n",
        "    # Test 1: Right-to-Left Consistency\n",
        "    print(\"RTL Test:\", generate_safe(\"اردو میں لکھیں: میرا نام علی ہے\"))\n",
        "\n",
        "    # Test 2: Poet Attribution\n",
        "    print(\"\\nPoet Test:\", generate_safe(\n",
        "        \"Who wrote this ghazal? دل سے جو بات نکلتی ہے اثر رکھتی ہے\"\n",
        "    ))\n",
        "\n",
        "    # Test 3: Mixed Script Handling\n",
        "    print(\"\\nMixed Script Test:\", generate_safe(\n",
        "        \"Convert: میں use کرتا ہوں Roman Urdu کو\"\n",
        "    ))\n",
        "\n",
        "test_urdu_features()"
      ],
      "metadata": {
        "id": "f6KdTxYG2uwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "# !pip install transformers torch\n",
        "\n",
        "# Import libraries\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load the DeepSeek-R1 model and tokenizer\n",
        "model_name = \"deepseek-ai/deepseek-llm-7b-chat\"  # Replace with the correct model path if not on Hugging Face\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Define an Urdu input prompt\n",
        "urdu_prompt = \"محبت کے بارے میں ایک شعر کہو\"  # \"Write a poem about love\"\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer(urdu_prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generate text\n",
        "outputs = model.generate(\n",
        "    inputs.input_ids,\n",
        "    max_length=100,  # Adjust the length of the generated text\n",
        "    num_return_sequences=1,  # Number of responses to generate\n",
        "    no_repeat_ngram_size=2,  # Avoid repeating phrases\n",
        "    top_k=50,  # Sampling parameter\n",
        "    top_p=0.95,  # Nucleus sampling parameter\n",
        "    temperature=0.7,  # Controls randomness\n",
        ")\n",
        "\n",
        "# Decode and print the generated Urdu text\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"Generated Urdu Text:\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "1iNJ-bgU0jFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama.from_pretrained(\n",
        "\trepo_id=\"bartowski/Tower-Babel_Babel-9B-Chat-GGUF\",\n",
        "\tfilename=\"Tower-Babel_Babel-9B-Chat-Q8_0.gguf\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "swRYl32eGOAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a chat prompt in Urdu\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"محبت کے بارے میں ایک شعر کہو\"  # \"Write a poem about love\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Generate a response\n",
        "response = llm.create_chat_completion(\n",
        "    messages=messages,\n",
        "    max_tokens=100,  # Maximum number of tokens to generate\n",
        "    temperature=0.7,  # Controls randomness\n",
        "    top_p=0.95,  # Nucleus sampling parameter\n",
        ")\n",
        "\n",
        "# Print the generated response\n",
        "print(\"Generated Urdu Text:\")\n",
        "print(response[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "id": "Qft0cgT6INaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "device = \"cuda\" # the device to load the model onto\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "  \"Tower-Babel/Babel-83B-Chat\",\n",
        "  torch_dtype=torch.bfloat16,\n",
        "  device_map=device\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Tower-Babel/Babel-83B-Chat\")\n",
        "\n",
        "# prepare messages to model\n",
        "prompt = \"Hiii How are you?\"\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "\n",
        "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
        "print(f\"Formatted text:\\n {text}\")\n",
        "print(f\"Model input:\\n {model_inputs}\")\n",
        "\n",
        "generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512, do_sample=True, eos_token_id=tokenizer.eos_token_id)\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "print(f\"Response:\\n {response[0]}\")\n"
      ],
      "metadata": {
        "id": "C0n8B_SSMjO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Memory Error"
      ],
      "metadata": {
        "id": "waM2bSum6GCy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}